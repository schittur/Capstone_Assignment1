---
title: "Coursera Capstone Project First Milestone Report"
author: Sriram Chitturi
output: html_document
---

This report is for the Week2 Assignment of Capstone project for Datascience course on Coursera.

The objective is to do some exploratory analysis of the sample dataset provided by SwiftKey for the project at https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

The dataset consists of text files from the following sources - blogs, news and twitter feeds.

In the current report we are using the files for US english language only found under en_US subfolder.

### Source Code
The source code for this presentation can be found at
https://github.com/schittur/Capstone_Assignment1/blob/master/Assignment1.Rmd


### Following table shows the file sizes and word counts in the corpus

```{r, echo=F, warning=F, message=F}
countwords <- function(line) {
    words <- unlist(strsplit(line, split = "\\W+"));
    l = length(words)
    l
}
filedetails <- function(filename, folder) {
    filepath = paste(folder, filename, sep = '/');
    fi <- file.info(filepath);
    con <- file(filepath, open = 'r');
    lines <- readLines(con, encoding = 'UTF-8');
    memsize = as.integer(object.size(lines));
    close(con);
    numlines = length(lines);
    cwords = unlist(lapply(lines, countwords));
    numwords = sum(cwords);
    remove(lines);
    data.frame(FileName = filename, FileSizeInBytes = as.integer(fi[, 'size']),
                    SizeInMemory = as.integer(memsize),
                    LineCount=numlines, WordCount=numwords, stringsAsFactors = F)
}

printfiledetails <- function(folder) {
    filenames <- list.files(folder)
    df <- lapply(filenames, filedetails, folder = folder);
    df <- do.call("rbind", df)
    print(df);
}

corpus_dir <- "H://Datascience/Capstone/final/en_US";
printfiledetails(corpus_dir)
```


### Resources constraint and sampling

It is taking a long time to process all these files on my laptop, so I am ***processing only a sample of 15,000 lines from each file***.

```{r, echo=FALSE, warning=F, message=F}
loadSampleFromAFile <- function(filename, folder) {
    filepath = paste(folder, filename, sep = '/');
    fi <- file.info(filepath);
    con <- file(filepath, open = 'r');
    lines <- readLines(con, encoding = 'UTF-8');
    s <- sample(unlist(lines), 15000);
    iconv(s, 'UTF-8', 'ASCII', "byte")
}
loadSampleFromAllFiles <- function(folder) {
    filenames <- list.files(folder)
    slines <- lapply(filenames, loadSampleFromAFile, folder = folder);
    names(slines) <- filenames;
    slines
}
slines <- loadSampleFromAllFiles(corpus_dir);

# slines <- unlist(slines)
# convert all odd characters to ascii
# slines <- iconv(slines, 'UTF-8', 'ASCII', "byte")

```

### Loading and cleaning data
From here on I have used the 'tm' package to do some exploratory analysis.

Steps required for cleaning data are to 

* load the Corpus from the sample lines collected and then 
  + remove numbers
  + remove punctuation marks
  + remove any stop words from english language like 'the', 'a', 'i' etc.. 
  + remove bad words
  + convert the text to lowercase to maintain uniformity

### Profanity filtering
To address profanity, I downloaded badwords from http://www.bannedwordlist.com/lists/swearWords.csv and removed
them from the words list. 

``` {r, echo=F, warning=F, message=F}
library(tm) # load text mining library
docs <- Corpus(VectorSource(slines));
# clean documents using all available transformations
docs <- tm_map(docs, removeNumbers);
docs <- tm_map(docs, removePunctuation);
#now convert to lowercase before processing english words
docs <- tm_map(docs, content_transformer(tolower));
docs <- tm_map(docs, removeWords, stopwords("english"));

# load a list of banned words, one suggested list is here
badwords = read.csv("http://www.bannedwordlist.com/lists/swearWords.csv");
docs <- tm_map(docs, removeWords, badwords);
docs <- tm_map(docs, stripWhitespace);

# create a document term matrix
docs <- tm_map(docs, PlainTextDocument);
```

### Exploratory Analysis

By using the 'wordcloud' package we can see the words most used.

For BLOGS

```{r, echo=F, warning=F, message=F}
library(wordcloud);
wordcloud(docs[1], max.words = 100, colors = brewer.pal(8, 'Dark2'))
```

For NEWS

```{r, echo=F, warning=F, message=F}
wordcloud(docs[2], max.words = 100, colors = brewer.pal(8, 'Dark2'))
```

For TWITTER

```{r, echo=F, warning=F, message=F}
wordcloud(docs[3], max.words = 100, colors = brewer.pal(8, 'Dark2'))
```

### Creating document term and term document matrices
```{r, echo=F, warning=F, message=F}
# start reading document in plain text
dtm <- DocumentTermMatrix(docs);
tdm <- TermDocumentMatrix(docs);
```
Before removing sparsity
```{r, echo=F, warning=F, message=F}
dtm
```
After removing sparsity
```{r, echo=F, warning=F, message=F}
dtm <- removeSparseTerms(dtm, 0.01);
dtm
```


### Frequency of words. The list below is the 20 most frequently used words.
```{r, echo=F, warning=F, message=F}
word_frequencies <- colSums(as.matrix(dtm));
top20 <- word_frequencies[tail(order(word_frequencies), n=20)];
top20
```

### Plotting the top 20 words frequencies
```{r, echo=F, warning=F, message=F}
forplot <- data.frame(word=names(top20), freq=top20)
library(ggplot2)   
p <- ggplot(forplot, aes(word, freq))
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p
```